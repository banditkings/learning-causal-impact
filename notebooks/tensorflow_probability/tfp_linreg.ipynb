{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "ndims = 1\n",
    "ndata = 100\n",
    "X = np.random.randn(ndata, ndims)\n",
    "w_ = np.random.randn(ndims)  # hidden\n",
    "noise_ = 0.1 * np.random.randn(ndata)  # hidden\n",
    "\n",
    "y_obs = X.dot(w_) + noise_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to https://colcarroll.github.io/ppl-api/ for this example\n",
    "\n",
    "X_tensor = tf.convert_to_tensor(X, dtype='float32')\n",
    "\n",
    "@tf.function\n",
    "def target_log_prob_fn(w):\n",
    "    w_dist = tfd.Normal(loc=tf.zeros((ndims, 1)), scale=1.0, name=\"w\")\n",
    "    w_prob = tf.reduce_sum(w_dist.log_prob(w))\n",
    "    y_dist = tfd.Normal(loc=tf.matmul(X_tensor, w), scale=0.1, name=\"y\")\n",
    "    y_prob = tf.reduce_sum(y_dist.log_prob(y_obs.reshape(-1, 1)))\n",
    "    return w_prob + y_prob\n",
    "\n",
    "\n",
    "# Initialize the HMC transition kernel.\n",
    "num_results = 1000\n",
    "num_burnin_steps = 500\n",
    "adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "    tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=target_log_prob_fn,\n",
    "        num_leapfrog_steps=4,\n",
    "        step_size=0.01),\n",
    "    num_adaptation_steps=int(num_burnin_steps * 0.8))\n",
    "\n",
    "samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "    num_results=num_results,\n",
    "    num_burnin_steps=num_burnin_steps,\n",
    "    current_state=tf.zeros((ndims, 1)),\n",
    "    kernel=adaptive_hmc,\n",
    "    trace_fn=lambda _, pkr: pkr.inner_results.is_accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1000 samples, single parameter\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = {\"b0\": samples.numpy()[:,0,0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "arviz - WARNING - Shape validation failed: input_shape: (1, 1000), minimum_shape: (chains=2, draws=4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b0</th>\n",
       "      <td>1.892</td>\n",
       "      <td>0.011</td>\n",
       "      <td>1.875</td>\n",
       "      <td>1.914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  \\\n",
       "b0  1.892  0.011   1.875    1.914        0.0      0.0     479.0     340.0   \n",
       "\n",
       "    r_hat  \n",
       "b0    NaN  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good reference is: https://www.tensorflow.org/probability/examples/Modeling_with_JointDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "dtype = tf.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dfhogg = pd.DataFrame(np.array([[1, 201, 592, 61, 9, -0.84],\n",
    "                                 [2, 244, 401, 25, 4, 0.31],\n",
    "                                 [3, 47, 583, 38, 11, 0.64],\n",
    "                                 [4, 287, 402, 15, 7, -0.27],\n",
    "                                 [5, 203, 495, 21, 5, -0.33],\n",
    "                                 [6, 58, 173, 15, 9, 0.67],\n",
    "                                 [7, 210, 479, 27, 4, -0.02],\n",
    "                                 [8, 202, 504, 14, 4, -0.05],\n",
    "                                 [9, 198, 510, 30, 11, -0.84],\n",
    "                                 [10, 158, 416, 16, 7, -0.69],\n",
    "                                 [11, 165, 393, 14, 5, 0.30],\n",
    "                                 [12, 201, 442, 25, 5, -0.46],\n",
    "                                 [13, 157, 317, 52, 5, -0.03],\n",
    "                                 [14, 131, 311, 16, 6, 0.50],\n",
    "                                 [15, 166, 400, 34, 6, 0.73],\n",
    "                                 [16, 160, 337, 31, 5, -0.52],\n",
    "                                 [17, 186, 423, 42, 9, 0.90],\n",
    "                                 [18, 125, 334, 26, 8, 0.40],\n",
    "                                 [19, 218, 533, 16, 6, -0.78],\n",
    "                                 [20, 146, 344, 22, 5, -0.56]]),\n",
    "                   columns=['id','x','y','sigma_y','sigma_x','rho_xy'])\n",
    "\n",
    "\n",
    "## for convenience zero-base the 'id' and use as index\n",
    "dfhogg['id'] = dfhogg['id'] - 1\n",
    "dfhogg.set_index('id', inplace=True)\n",
    "\n",
    "## standardize (mean center and divide by 1 sd)\n",
    "dfhoggs = (dfhogg[['x','y']] - dfhogg[['x','y']].mean(0)) / dfhogg[['x','y']].std(0)\n",
    "dfhoggs['sigma_y'] = dfhogg['sigma_y'] / dfhogg['y'].std(0)\n",
    "dfhoggs['sigma_x'] = dfhogg['sigma_x'] / dfhogg['x'].std(0)\n",
    "\n",
    "# def plot_hoggs(dfhoggs):\n",
    "#   ## create xlims ylims for plotting\n",
    "#   xlims = (dfhoggs['x'].min() - np.ptp(dfhoggs['x'])/5,\n",
    "#            dfhoggs['x'].max() + np.ptp(dfhoggs['x'])/5)\n",
    "#   ylims = (dfhoggs['y'].min() - np.ptp(dfhoggs['y'])/5,\n",
    "#            dfhoggs['y'].max() + np.ptp(dfhoggs['y'])/5)\n",
    "\n",
    "#   ## scatterplot the standardized data\n",
    "#   g = sns.FacetGrid(dfhoggs, size=8)\n",
    "#   _ = g.map(plt.errorbar, 'x', 'y', 'sigma_y', 'sigma_x', marker=\"o\", ls='')\n",
    "#   _ = g.axes[0][0].set_ylim(ylims)\n",
    "#   _ = g.axes[0][0].set_xlim(xlims)\n",
    "\n",
    "#   plt.subplots_adjust(top=0.92)\n",
    "#   _ = g.figure.suptitle('Scatterplot of Hogg 2010 dataset after standardization', fontsize=16)\n",
    "#   return g, xlims, ylims\n",
    "\n",
    "# g = plot_hoggs(dfhoggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = dfhoggs['x'].values\n",
    "sigma_y_np = dfhoggs['sigma_y'].values\n",
    "Y_np = dfhoggs['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdl_ols = tfd.JointDistributionSequential([\n",
    "#     # b0 ~ Normal(0, 1)\n",
    "#     tfd.Normal(loc=tf.cast(0, dtype), scale=1.),\n",
    "#     # b1 ~ Normal(0, 1)\n",
    "#     tfd.Normal(loc=tf.cast(0, dtype), scale=1.),\n",
    "#     # x ~ Normal(b0+b1*X, 1)\n",
    "#     lambda b1, b0: tfd.Normal(\n",
    "#       # Parameter transformation\n",
    "#       loc=b0 + b1*X_np,\n",
    "#       scale=sigma_y_np)\n",
    "# ])\n",
    "\n",
    "mdl_ols_ = tfd.JointDistributionSequential([\n",
    "    # b0\n",
    "    tfd.Normal(loc=tf.cast(0, dtype), scale=1.),\n",
    "    # b1\n",
    "    tfd.Normal(loc=tf.cast(0, dtype), scale=1.),\n",
    "    # likelihood\n",
    "    #   Using Independent to ensure the log_prob is not incorrectly broadcasted\n",
    "    lambda b1, b0: tfd.Independent(\n",
    "        tfd.Normal(\n",
    "            # Parameter transformation\n",
    "            # b1 shape: (batch_shape), X shape (num_obs): we want result to have\n",
    "            # shape (batch_shape, num_obs)\n",
    "            loc=b0 + b1*X_np,\n",
    "            scale=sigma_y_np),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('b0', ()), ('b1', ()), ('x', ('b1', 'b0')))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_ols.resolve_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.Independent(\"IndependentNormal\", batch_shape=[], event_shape=[20], dtype=float64)\n",
      "tfp.distributions.Normal(\"Normal\", batch_shape=[20], event_shape=[], dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(mdl_ols_.sample_distributions()[0][-1])\n",
    "print(mdl_ols.sample_distributions()[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=-0.06748552886278159>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Root = tfd.JointDistributionCoroutine.Root  # Convenient alias.\n",
    "def model():\n",
    "  b1 = yield Root(tfd.Normal(loc=tf.cast(0, dtype), scale=1.))\n",
    "  b0 = yield Root(tfd.Normal(loc=tf.cast(0, dtype), scale=1.))\n",
    "  yhat = b0 + b1*X_np\n",
    "  likelihood = yield tfd.Independent(\n",
    "        tfd.Normal(loc=yhat, scale=sigma_y_np),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    )\n",
    "\n",
    "mdl_ols_coroutine = tfd.JointDistributionCoroutine(model)\n",
    "mdl_ols_coroutine.log_prob(mdl_ols_coroutine.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructTuple(\n",
       "  var0=<tf.Tensor: shape=(), dtype=float64, numpy=0.8899468403586868>,\n",
       "  var1=<tf.Tensor: shape=(), dtype=float64, numpy=0.7728553511496148>,\n",
       "  var2=<tf.Tensor: shape=(20,), dtype=float64, numpy=\n",
       "    array([ 1.84876111,  1.64697786, -1.46597035,  2.52133377,  1.18839469,\n",
       "           -1.07816046,  0.8359043 ,  1.05324044,  1.55456861,  0.6797094 ,\n",
       "            0.5811698 ,  1.51899324, -0.39635904,  0.12090469,  0.3083027 ,\n",
       "            0.96955596,  0.53666496, -0.17332018,  1.37816081,  0.38455846])>\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_ols_coroutine.sample()  # output is a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc-statespace-W4oADPxQ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
